{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cIBIolg3gB3P",
        "8DGUpjxxziG0",
        "Qv9u6m90gEA0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Árboles de Decisión**\n",
        "A diferencia de otros algoritmos de aprendizaje supervisado que hemos visto hasta ahora, los **Árboles de Decisión** (*Decision Trees*) son herramientas no paramétricas, es decir, no es necesario entrenar y afinar unos parámetros que recogan la información que el modelo vaya aprendiendo. Aun así, los Árboles de Decision y sus derivados son de los algoritmos más populares tanto para tareas de clasificación como regresión."
      ],
      "metadata": {
        "id": "c48ca5F1FL3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Métricas de impureza**"
      ],
      "metadata": {
        "id": "cIBIolg3gB3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los Árboles de Decisión para clasificación funcionan de forma iterativa, dividiendo el dataset según un criterio o decisión sobre los valores de las columnas. Por ejemplo, supongamos un dataset que recoge las características de diferentes tipos de setas, tales como la altura del tallo o la estación del año en que aparecen. Nuestro dataset además nos indica si las setas son o no venenosas, columna que utilizaremos como objetivo de clasificación. Independientemente de si se trata de una tarea de clasificación o de regresión, el Árbol de Decisión comienza siempre realizando preguntas binarias (de sí o no) a partir de alguna de las columnas, intentando dividir los datos de la forma más pura posible. Una primera pregunta podría ser \"¿Es el diámetro de la copa mayor de 6 cm?\" lo que supondría la partición del dataset en dos subconjuntos u \"nodos\", en función de si cumplen o no con la condición. Sobre cada una de las nodos el algoritmo realizaría una segunda partición, usando criterios independientes para cada una de las \"ramas\" (de ahí el nombre dado a este tipo de métodos). Así procedería continuando con las particiones hasta que se cumpla cierto criterio de parada.\n",
        "\n",
        "\n",
        "Sin embargo queda por responder una pregunta fundamental... ¿Qué criterio utilizar para realizar la partición? ¿Por qué elegir un diámetro de copa mayor de 6 cm y no 4? ¿O porqué no cualquier otro criterio como si son o no setas de temporada? Lo cierto es que el algoritmo debe comprobar con todas las opciones posibles, escogiendo aquellas particiones que devuelvan nodos con menor impureza. Una impureza baja implica que los dos conjuntos u nodos resultantes están bien separados o, siguiendo el ejemplo, uno de ellos contiene una amplia mayoría de setas venenosas mientras que el otro no. Una impureza alta supone que esa separación no ha sido del perfecta y los dos conjuntos contienen una mezcla muy similar de datos. Existen muchas formas de calcular la impureza, pero dos son los métodos más utilizados:\n",
        "\n",
        "- **Coeficiente Gini**: es probablemente la opción más utilizada. Si el ratio de pertenencia a la clase es igual a $p_{i}$, mide la probabilidad de que un dato de esa clase sea clasificado incorrectamente ($1-p_{i}$). El Coeficiente de Gini es el sumatorio de estas probabilidades para cada clase $d$:\n",
        "$$Gini(p) = \\sum_{i=1}^{d}p_{i}(1-p_{i})$$\n",
        "  o también:\n",
        "  $$Gini(p) = 1 - \\sum_{i=1}^{d}p_{i}^{2}$$\n",
        "  Toma valores entre $0$ y $0.5$, siendo $0$ un indicativo de máxima pureza, es decir\n",
        "- **Entropía**: es una medida de la incertidumbre de los datos. Se calcula la suma, ponderada por la probabilidad, de la información de un evento. La información es un concepto angular dentro del mundo de la computación y mide la lo sorpresivo que es un evento. Es por ello que la información es el inverso de la probabilidad (en logaritmo).\n",
        "$$H(p) = \\sum_{i=1}^{d}p_{i}\\times log(\\frac{1}{p_{i}})$$\n",
        "\n",
        "  o de forma equivalente:\n",
        "$$H(p) = - \\sum_{i=1}^{d}p_{i}\\times log(p_{i})$$\n",
        "  En útima instancia, la entropía será más alta hasta $1$ cuando las dos alternativas sean igualmente probables, y bajará hasta $0$ cuando tengamos menos incertidumbre sobre el resultado.\n",
        "\n",
        "La realidad es que ambas métricas devuelven resultados muy similares, por lo que muchas veces ni siquiera se toman como un posible meta-parámetro."
      ],
      "metadata": {
        "id": "flgbmMQRF8hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Construcción del árbol**"
      ],
      "metadata": {
        "id": "8DGUpjxxziG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indistintamente de las métricas utilizadas, nos interesa una partición que devuelva nodos lo más homogéneos posibles. Así que el objetivo es encontrar un criterio que minimice cualquiera de estos coeficientes. El cálculo de estas métricas se realiza para cada hoja o nodo. Para obtener el coeficiente de la partición total simplemente lo ponderamos por el número de elementos de cada nodo. Veamoslo con el ejemplo.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GVTIFy5MnAwJEUPZvglrp89M7hYFL_Wb\" width=\"300\">\n",
        "\n",
        "Así, el Coficiente Gini ponderado de la partición completa es.\n",
        "\n",
        "$$\\frac{21}{47}0.499 + \\frac{26}{47}0.141 = 0.301$$\n",
        "\n",
        "Para decidirnos por esta partición (*season = 1* en el ejemplo) este Coeficiente de Gini resultante debe ser menor que el de otras posibles particiones. Cada nodo resultante es un subconjunto de datos, el cual puede a sí mismo dividirse siguiendo otros criterios. De esta manera se contruye cada rama del árbol. Una vez que el árbol se detiene podemos calcular su impureza total, que sería la misma operación de suma de los métricas de impureza de cada nodo terminal ponderadas por el número de muestras en esos nodos. Es esperable que en cada partición, ese total de impureza sea cada vez menor, aunque también suele ocurrir que el descenso es cada vez menos intenso.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Os0liaFuzQzSEt2rQHYk00SZXD5UmaZQ\" width=\"500\">\n",
        "\n",
        "\n",
        "Existe una versión alternativa de Árbol de Decisión para tareas de regresión (*Regression Decision Trees*). La posibilidad de ser usados tanto para regresión como clasificación les ha dado a los Árboles de Decisión el sobrenombre de CART (*Classification and Regression Trees*). Al igual que en la clasificación, para tareas de regresión se comienza realizando una primera partición, es decir, eligiendo una variable y un valor que seccione el conjunto de datos en dos. La diferencia respecto al árbol de clasificación es que en este caso ya no haremos uso de métricas de impureza, sino la tradicional suma de las distancias al cuadrado, suma cuya minimización será el criterio para elegir uno u otro criterio de partición. La predicción sería simplemente la media de los valores objetivo en cada uno de los subconjuntos. También podríamos continuar ramificando el modelo, encontrando un nuevo criterio de partición para cada uno de los subconjuntos. La predicción de una instancia se realiza asignandola iterativamente a cada subconjunto anidado, tomando la media del nodo final al que pertenece. Como vemos en la imagen, los Árboles de Decisión para regresión permiten modelar estructuras no lineales en los datos.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ayAGQ5yMo6I8W_Tncg2aUkddw65tFLFq\" width=\"500\">\n",
        "\n",
        "Ejemplo de *Regression Decision Tree* en ScikitLearn: https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression\n",
        "\n",
        "\n",
        "\n",
        "<br>Por último, los Árboles de Decisión permiten también la clasificación multiple (*Multi-Output Decision Trees*), es decir, predecir varias columnas al mismo tiempo sin necesidad de crear algoritmos diferentes para cada una. Para ello se aprovecha de la correlación entre las diferentes salidas. Continuando con el ejemplo, nuestro árbol podría realizar una partición en la que no solo separase entre setas que son o no venenosas, sino si son o no comestibles, una característica que evidentemente correlaciona con la primera columna objetivo. Para realizar esto simplemente ejecuta la partición y calcula las predicciones para cada columna.\n",
        "\n",
        "\n",
        "Ejemplo de *Multi-Output Decision Tree* en ScikitLearn: https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput"
      ],
      "metadata": {
        "id": "XUNbHZUuzszu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Criterios de parada y poda del árbol**"
      ],
      "metadata": {
        "id": "JpbqvuPKOfxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A estas alturas ya sabréis que estamos ante un modelo de aprendizaje supervisado. Y como buen modelo supervisado siempre estará el peligro de sobreajustar con un árbol excesivamente complejo, con demasiadas ramificaciones y nodos. Sin ningun criterio de parada, el algoritmo del árbol continúa haciendo particiones hasta obtener clases totalmente homogéneas, resultado que casi nunca es deseable de cara a generalizar el aprendizaje. Es por eso que son necesarios criterios de parada para no aumentar excesivamente la complejidad del árbol.\n",
        "\n",
        "Algunos **criterios de parada** pueden ser limitar la profundidad del árbol (entendiendo profundidad como particiones anidadas), exigir un número mínimo de muestras en un subconjunto para ejecutar una partición o limitar las particiones a nodos que consigan una disminución de impureza superior a un umbral, por debajo del cual no consideramos necesaria realizar la partición.\n",
        "\n",
        "Además de criterios de parada pueden aplicarse **criterios de poda** (*post pruning*). Esto es, una vez que el algoritmo del Árbol de Decisión este completo, podemos podar algunas ramas para reducir su complejidad. El criterio más conocido es el de Poda de Mínimo Coste-Complejidad (*Minimal Cost-Complexity Pruning*), definido formalmente siguiente la siguiente fórmula:\n",
        "\n",
        "$$C_{\\alpha}(T) = R(T) + \\alpha\\left | T \\right |$$\n",
        "\n",
        "siendo $C$ la llamada medida de Coste-Complejidad, $R$ la medida de error del modelo del árbol $T$ (también definida como la impureza promedio de las particiones finales) y $\\left | T \\right |$ el número de nodos. Esta fórmula se encuentra parametrizada por $\\alpha$ que controla el balanceo entre el error de predicción y la complejidad del árbol. Cuanto mayor $\\alpha$ mayor la penalización del número de nodos, al elevar la medida de Coste-Complejidad y siendo esta una métrica que buscamos minimizar. Así, podría darse el caso de que dos árboles alternativos, aun teniendo uno de ellos un $R$ o error mayor, su métrica $C$ fuese más baja al tener un número menor de nodos, siendo por tanto preferible al segundo.\n",
        "\n",
        "Ejemplo de Poda de Coste-Complejidad en ScikitLearn: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning"
      ],
      "metadata": {
        "id": "puADrgVMOlCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Guía de uso de Árboles de Decisión**"
      ],
      "metadata": {
        "id": "fVG0MNzXdq63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo de *Decision Tree* es realmente muy versátil, capaz de realizar tareas tanto de clasificación como regresión. Acepta valores tanto categóricos codificados como numéricos, sin necesidad de estandarización. Permite además reconocer estructuras no lineales en los datos. Sin embargo, su característica más destacable es su interpretabilidad. Se trata de un método de \"caja blanca\": en todo momento podemos saber la ruta de decisiones booleanas que ha tomado para realizar cualquier clasificación.\n",
        "\n",
        "Algunas limitaciones son su tendencia hacia el sobreajuste. Pequeñas modificaciones en el dataset pueden dar lugar a árboles significativamente distintos. Sin embargo, existen versiones algo más complejas de los Árboles de Decisión que permiten solventar mayormente estos inconvenientes. Veremos más adelante estas alternativas.\n",
        "\n",
        "Es posible que, al menos para tareas de clasificación, el principal tratamiento previo de los datos sea el de crear clases objetivo balanceadas, es decir, que los datos de entrada contengan un número similar de muestras para cada clase."
      ],
      "metadata": {
        "id": "-IBxhCq5dw_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En ScikitLearn podremos encontrarnos con varios meta-parámetros con los que afinar nuestros modelos:\n",
        "- `min_samples_split`: número mínimo de muestras necesarias para dividir un nodo. Un valor más alto puede evitar el sobreajuste. Valores más bajos implican el desarrollo total del árbol.\n",
        "- `max_depth`: profundidad máxima del árbol. Controla la profundidad del árbol ayuda a prevenir el sobreajuste. Los árboles se desarrollan hasta que cada hoja contenga menos de `min_samples_split` muestras. Se recomienda un valor `None` para que el parámetro sea controlado por `min_samples_split`.\n",
        "- `min_samples_leaf`: número mínimo de muestras para que se considere un nodo. Normalmente se utiliza como versión alternativa al metaparámetro `min_samples_leaf`.\n",
        "- `min_impurity_decrease`: se realizará una partición si esta induce una disminución de la impureza mayor o igual a este valor.\n",
        "- `ccp_alpha`: parámetro $\\alpha$ para la Poda por Coste-Complejidad. Cuanto mayor es este parámetro, la penalización por número de nodos es mayor y ayuda a reducir el sobreajuste."
      ],
      "metadata": {
        "id": "mYldnJo3tbaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análsis del DataFrame**"
      ],
      "metadata": {
        "id": "Qv9u6m90gEA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from typing import List, Tuple, Dict\n",
        "import seaborn as sns\n",
        "\n",
        "url='https://drive.google.com/file/d/1btCzqdC5EHUdpJbICCdouOUAJQukx3mM/view?usp=sharing'\n",
        "file_id=url.split('/')[-2]\n",
        "dwn_url='https://drive.google.com/uc?id=' + file_id\n",
        "data = pd.read_csv(dwn_url)\n",
        "\n",
        "data.head(6)"
      ],
      "metadata": {
        "id": "wYDcte2pHA5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "diXJ1xnaXHzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_columns = data.columns.tolist()\n",
        "\n",
        "num_columns = data.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "print(\"Columnas numéricas: \", num_columns)\n",
        "\n",
        "cat_columns = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "print(\"Columnas categóricas: \", cat_columns)"
      ],
      "metadata": {
        "id": "fhTTMbUMXK4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = \"class\"\n",
        "\n",
        "pred_columns = [col for col in list_columns if col != target_column]\n",
        "num_pred_columns = [col for col in list_columns if col in num_columns]\n",
        "cat_pred_columns = [col for col in list_columns if col in cat_columns]\n",
        "print(\"Columnas predictoras: \", pred_columns)\n",
        "print(\"Columnas numéricas predictoras: \", num_pred_columns)\n",
        "print(\"Columnas categóricas predictoras: \", cat_pred_columns)"
      ],
      "metadata": {
        "id": "K9yFIh7gXN7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().T"
      ],
      "metadata": {
        "id": "vbxUv-TqXSHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_classes = data[target_column].unique().tolist()"
      ],
      "metadata": {
        "id": "ORHAWdrBow9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "cPenpfyjXUZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = data[target_column].value_counts()\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.bar(class_counts.index, class_counts.values, color=['orange', 'purple'])\n",
        "plt.xlabel('Clase')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Frecuencia de clases objetivo')\n",
        "plt.xticks([0, 1], ['Clase 0', 'Clase 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DhTuyJRLqg34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clasificación binaria con Árbol de Decisión**"
      ],
      "metadata": {
        "id": "OrjylFOtYFgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# CREACIÓN DEL PIPELINE CON ÁRBOL DE DECISIÓN\n",
        "model = Pipeline([\n",
        "    ('classifier', DecisionTreeClassifier())  # Árbol de Decisión para tareas de clasificaición.\n",
        "])"
      ],
      "metadata": {
        "id": "iLhajHkjYE6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DIVISIÓN ENTRE COLUMNAS PREDICTIVAS Y OBJETIVO\n",
        "# Separar colunas predictoras de columna objetivo\n",
        "X = data.drop(columns=target_column)\n",
        "y = data[target_column]"
      ],
      "metadata": {
        "id": "65TVb84EcZUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
        "\n",
        "# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO\n",
        "# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Tamaño datos de entrenamiento:\", X_train.shape)\n",
        "print(\"Tamaño datos de testeo:\", X_test.shape)"
      ],
      "metadata": {
        "id": "Re1_ipKdccdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIÓN DE LA BÚSQUEDA DE HIPERPARÁMETROS\n",
        "# Configurar la búsqueda por validación cruzada para encontrar el mejor valor de alpha\n",
        "metaparameter_list = ['classifier__max_depth', 'classifier__min_samples_split', 'classifier__criterion']\n",
        "param_grid = {\n",
        "    metaparameter_list[0]: [3, 6, 10, 20],\n",
        "    metaparameter_list[1]: [2, 5, 10],\n",
        "    metaparameter_list[2]: [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "# Configurar el GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, verbose = 1, scoring='f1_weighted')"
      ],
      "metadata": {
        "id": "_oKCmq0mdIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# BÚSQUEDA DE HIPERPARÁMETROS CON VALIDACIÓN ANIDADA.\n",
        "# Este paso es independiente del entrenamiento posterior, pero es recomendable para evaluar la variabilidad de los modelos en cuanto a evaluación y selección de hiperparámetros.\n",
        "n_splits = 5\n",
        "outer_cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "best_params_list = []\n",
        "best_scores = []\n",
        "\n",
        "# Aquí estamos haciendo una Validación Cruzada manualmente. En cada iteración (tantas como número de folds indicados) se calculará un GridSearchCV\n",
        "# con el conjunto de entrenamiento seleccionado en ese fold.\n",
        "for train_index, test_index in outer_cv.split(X_train):\n",
        "    X_train_fold = X_train.iloc[train_index]\n",
        "    X_test_fold = X_train.iloc[test_index]\n",
        "    y_train_fold = y_train.iloc[train_index]\n",
        "    y_test_fold = y_train.iloc[test_index]\n",
        "\n",
        "    # Ejecutar GridSearchCV\n",
        "    grid_search.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Almacenar los mejores parámetros y los mejores resultados en cada split\n",
        "    best_params_list.append(grid_search.best_params_)\n",
        "    best_scores.append(grid_search.best_score_)\n",
        "\n",
        "# Un bucle por fold, indicando el hiperparámetro óptimo y la mejor métrica de error de esa iteración.\n",
        "for split in range(n_splits):\n",
        "  for metaparameter in metaparameter_list:\n",
        "    value = best_params_list[split][metaparameter]\n",
        "    if isinstance(value, (int, float)):\n",
        "        print(f'Mejor valor en el fold {split} del {metaparameter} en VC anidada: {round(value, 3)}')\n",
        "    else:\n",
        "        print(f'Mejor valor en el fold {split} del {metaparameter} en VC anidada: {value}')\n",
        "  print(f\"Mejor f1-Score ponderada en el fold {split}: {np.round(best_scores[split],2)}\\n\")\n",
        "\n",
        "# Un bucle para los estadísticos de cada hiperparámetro\n",
        "for metaparameter in metaparameter_list:\n",
        "  values = [value[metaparameter] for value in best_params_list] # Recogemos los diferentes valores que nos devuelve cada fold\n",
        "  if isinstance(values[0], (int, float)):\n",
        "    mean = sum(values) / len(values) # Calculamos la media\n",
        "    std = np.sqrt(sum((value - mean) ** 2 for value in values) / len(values)) # Calculamos la Desviación Típica\n",
        "    print(f'Promedio de las puntuaciones {metaparameter} en VC anidada: {round(mean,3)}')\n",
        "    print(f'Desviación Típica de las puntuaciones {metaparameter} en VC anidada: {round(std,3)}\\n')\n",
        "  else:\n",
        "    unique_values, counts = np.unique(values, return_counts=True) # Valores y su frecuencia en values\n",
        "    max_index = np.argmax(counts) # Índice del valor con mayor frecuencia\n",
        "    mode = unique_values[max_index]\n",
        "    count = counts[max_index]\n",
        "    print(f'Moda de las puntuaciones {metaparameter} en VC anidada: {mode}')\n",
        "    print(f'Frecuencia de la moda de las puntuaciones {metaparameter} en VC anidada: {count}')"
      ],
      "metadata": {
        "id": "W88INXqgdTAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BÚSQUEDA DE HIPERPARÁMETROS SIN VALIDACIÓN ANIDADA\n",
        "# Entrenamos ahora con GridSearchCV sin anidar.\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mostrar mejor puntuación y los mejores parámetros. Podemos compararlo a aquellos valores obtenidos en la validación cruzada anidada.\n",
        "for metaparameter in metaparameter_list:\n",
        "  value = grid_search.best_params_[metaparameter]\n",
        "  if isinstance(value, (int, float)):\n",
        "    print(f'Mejor puntuación del {metaparameter} en VC anidada: {round(grid_search.best_params_[metaparameter],3)}')\n",
        "  else:\n",
        "    print(f'Mejor valor del {metaparameter} en VC anidada: {value}')\n",
        "print(\"Mejor F1-Score:\", np.round(grid_search.best_score_,2))"
      ],
      "metadata": {
        "id": "VyGTWh_YdVXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTEO DEL MODELO\n",
        "# Recoger el modelo con los mejores hiperparámetros\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "DDvv1mHGdW4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "# EVALUACIÓN DEL MODELO\n",
        "accuracy = round(accuracy_score(y_test, y_pred), 2)\n",
        "# Para la precisión, recall, y f1, especificamos 'weighted'. Esto es útil pues pondera las clases, permitiéndonos hacer una idea de la calidad de la clasificación para clases desbalanceadas.\n",
        "precision = round(precision_score(y_test, y_pred), 2)\n",
        "recall = round(recall_score(y_test, y_pred), 2)\n",
        "f1 = round(f1_score(y_test, y_pred), 2)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Exactitud:\", accuracy)\n",
        "print(\"Precisión ponderada:\", precision)\n",
        "print(\"Sensibilidad ponderada:\", recall)\n",
        "print(\"F1 Score ponderada:\", f1)\n",
        "print(\"Matriz de Confusión:\\n\", conf_matrix)"
      ],
      "metadata": {
        "id": "M_SxfQdzdY_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracción del Árbol de Decisión del pipeline\n",
        "tree_model = best_model.named_steps['classifier']\n",
        "path = tree_model.cost_complexity_pruning_path(X_train, y_train) #  Recogemos los coeficientes alfa y las correspondientes impurezas totales en cada paso del proceso de poda.\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax.set_xlabel(\"Coeficiente alpha\")\n",
        "ax.set_ylabel(\"Impureza total del árbol\")\n",
        "ax.set_title(\"Impureza total vs Coeficiente alpha\")"
      ],
      "metadata": {
        "id": "LE3IHhb59A0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -qq graphviz\n",
        "!pip install -q graphviz pydotplus"
      ],
      "metadata": {
        "id": "zpGt73yHmAZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "# INTERPRETACIÓN DEL MODELO\n",
        "# Crear un archivo DOT\n",
        "dot_data = export_graphviz(tree_model,\n",
        "                           out_file = None,\n",
        "                           feature_names = pred_columns, # Nombres de las columnas predictivas\n",
        "                           class_names = [str(x) for x in target_classes],  # Nombres de las clases a predecir. Es necesario asegurarse que sean strings.\n",
        "                           filled = True,\n",
        "                           rounded = True,\n",
        "                           special_characters = True)\n",
        "\n",
        "# Visualizar el árbol usando graphviz y pydotplus en Colab\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "iOzP8mcsmEm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}